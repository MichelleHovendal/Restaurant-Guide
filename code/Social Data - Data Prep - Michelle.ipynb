{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project B\n",
    "\n",
    "\n",
    "This notebook will cover only cover the data prep and merge of the two larger json files.\n",
    "\n",
    "## Packages Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites\\\\data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites\\\\data'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Business Dataset + Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160585, 14)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('yelp_academic_dataset_business.json', lines=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
       "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
       "       'attributes', 'categories', 'hours'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to look at the restaurants that are open and not closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['is_open']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['is_open'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by select all the rows that mention restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['categories'].str.contains('Restaurants',\n",
    "              case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explode = df.assign(categories = df.categories\n",
    "                         .str.split(', ')).explode('categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Restaurants                  32022\n",
       "Food                         10732\n",
       "Nightlife                     5550\n",
       "Bars                          5345\n",
       "Sandwiches                    4795\n",
       "American (Traditional)        4379\n",
       "Fast Food                     4326\n",
       "Pizza                         3890\n",
       "Breakfast & Brunch            3872\n",
       "American (New)                3254\n",
       "Burgers                       3246\n",
       "Coffee & Tea                  3010\n",
       "Mexican                       2815\n",
       "Chinese                       2329\n",
       "Italian                       2130\n",
       "Seafood                       1998\n",
       "Salad                         1975\n",
       "Event Planning & Services     1916\n",
       "Cafes                         1871\n",
       "Japanese                      1773\n",
       "Name: categories, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_explode.categories.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['Food Delivery Services','Food Safety Training', 'Food Tours', \n",
    "          'Food Banks','Chinese Martial Arts','Traditional Chinese Medicine',\n",
    "         'Coffee & Tea Supplies','Japanese Curry', 'Food Court',\n",
    "         'Food Trucks','Food Stands']\n",
    "\n",
    "df = df[~df['categories'].str.contains('|'.join(remove),\n",
    "              case=False, na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to split the category column into different subgroups, so we can do some nice plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_kitchens = ['thai', 'chinese','japanese','korean','indian','american',\n",
    "                 'caribbean','italian','mediterranean','mexican', 'cajun',\n",
    "                'vietnamese','greek']\n",
    "\n",
    "cat_type = ['Food','Nightlife','Bars','Sandwiches','Pizza','Breakfast & Brunch', 'Fast Food',\n",
    "            'Burgers','Salad', 'Buffet', 'Cafes','Coffee & Tea', 'Vegetarian', 'Steakhouse', 'Sushi Bars',\n",
    "            'Diners','Wine Bars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explode = df.assign(categories = df.categories\n",
    "                         .str.split(', ')).explode('categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_kitchen = df_explode[df_explode['categories'].str.contains(\n",
    "              '|'.join(cat_kitchens),\n",
    "              case=False, na=False)]\n",
    "\n",
    "df_cat_kitchen = df_cat_kitchen.rename(columns={'categories':'cat_kitchen'})\n",
    "df_cat_kitchen = df_cat_kitchen['cat_kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_type = df_explode[df_explode['categories'].str.match(\n",
    "              '|'.join(cat_type),\n",
    "              case=True, na=False)]\n",
    "\n",
    "df_cat_type = df_cat_type.rename(columns={'categories':'cat_type'})\n",
    "df_cat_type = df_cat_type['cat_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Food', 'Bars', 'Nightlife', 'Salad', 'Sandwiches', 'Cafes',\n",
       "       'Vegetarian', 'Breakfast & Brunch', 'Pizza', 'Sushi Bars',\n",
       "       'Steakhouses', 'Fast Food', 'Burgers', 'Coffee & Tea', 'Diners',\n",
       "       'Buffets', 'Wine Bars'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_list = [df, df_cat_kitchen, df_cat_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = reduce(lambda  left, right: pd.merge(left,right,left_index=True,\n",
    "                                            right_index=True), df_merge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29836, 15)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CO', 'FL', 'MA', 'OR', 'OH', 'GA', 'TX', 'BC', 'WA', 'KS', 'KY'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a for-loop and if-statement, this is used to insert the full name of the state. It is assumed that not all people know what state MA is. Furthermore, the latitude and longitude for each state is also inserted. These are used later to give us an overview over where we have data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "long = []\n",
    "lat = []\n",
    "for state in df_merged['state']:\n",
    "    if state =='MA':\n",
    "        name.append('Massachusetts')\n",
    "        lat.append(42.407211)\n",
    "        long.append(- 71.382439)\n",
    "    elif state =='FL':\n",
    "        name.append('Florida')\n",
    "        lat.append(27.994402)\n",
    "        long.append(-81.760254)\n",
    "    elif state =='OR':\n",
    "        name.append('Oregon')\n",
    "        lat.append(44.000000)\n",
    "        long.append(-120.500000)\n",
    "    elif state =='BC':\n",
    "        name.append('British Columbia')\n",
    "        lat.append(53.726669)\n",
    "        long.append(-127.647621)\n",
    "    elif state =='GA':\n",
    "        name.append('Georgia')\n",
    "        lat.append(33.247875)\n",
    "        long.append(-83.441162)\n",
    "    elif state =='TX':\n",
    "        name.append('Texas')\n",
    "        lat.append(31.000000)\n",
    "        long.append(-100.000000)\n",
    "    elif state =='OH':\n",
    "        name.append('Ohio')\n",
    "        lat.append(40.367474)\n",
    "        long.append(-82.996216)\n",
    "    elif state =='CO':\n",
    "        name.append('Colorado')\n",
    "        lat.append(39.113014)\n",
    "        long.append(-105.358887)\n",
    "    elif state =='WA':\n",
    "        name.append('Washington')\n",
    "        lat.append(47.751076)\n",
    "        long.append(-120.740135)\n",
    "    elif state =='KS':\n",
    "        name.append('Kansas')\n",
    "        lat.append(38.500000)\n",
    "        long.append(-98.000000)\n",
    "    elif state == 'KY':\n",
    "        name.append('Kentucky')\n",
    "        lat.append(37.839333)\n",
    "        long.append(-84.270020)\n",
    "        \n",
    "df_merged['state_name'], df_merged['latitude_state'], df_merged['longitude_state']  = name, lat, long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting some more features, e.g. pricerange on the restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['attributes'] = df_merged['attributes'].apply(lambda x: {} if x is None else x)\n",
    "df_att = pd.json_normalize(df_merged.attributes)\n",
    "df_att = df_att['RestaurantsPriceRange2'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29836, 20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PriceRange = []\n",
    "AvgPrice = []\n",
    "for row in df_att['RestaurantsPriceRange2']:\n",
    "    if row == '1' :    \n",
    "        PriceRange.append('$')\n",
    "        AvgPrice.append(1)\n",
    "    elif row == '2':   \n",
    "        PriceRange.append('$$')\n",
    "        AvgPrice.append(20)\n",
    "    elif row == '3':  \n",
    "        PriceRange.append('$$$')\n",
    "        AvgPrice.append(45)\n",
    "    elif row == '4':  \n",
    "        PriceRange.append('$$$$')\n",
    "        AvgPrice.append(75)\n",
    "    else:           \n",
    "        PriceRange.append('Unknown')A\n",
    "        AvgPrice.append(0)\n",
    "        \n",
    "df_merged['PriceRange'], df_merged['AvgPrice'] = PriceRange, AvgPrice\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$$         17575\n",
       "$           8268\n",
       "Unknown     2625\n",
       "$$$         1223\n",
       "$$$$         145\n",
       "Name: PriceRange, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['PriceRange'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Review Data (Massive dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500000\n",
    "review = pd.read_json('yelp_academic_dataset_review.json', lines=True,\n",
    "                      dtype={'review_id':str,'user_id':str,\n",
    "                             'business_id':str,'stars':int,\n",
    "                             'date':str,'text':str,'useful':int,\n",
    "                             'funny':int,'cool':int},\n",
    "                      chunksize=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416263 out of 500,000 related reviews\n",
      "384085 out of 500,000 related reviews\n",
      "371677 out of 500,000 related reviews\n",
      "358510 out of 500,000 related reviews\n",
      "349565 out of 500,000 related reviews\n",
      "356785 out of 500,000 related reviews\n",
      "353610 out of 500,000 related reviews\n",
      "407361 out of 500,000 related reviews\n",
      "371185 out of 500,000 related reviews\n",
      "362734 out of 500,000 related reviews\n",
      "361974 out of 500,000 related reviews\n",
      "362709 out of 500,000 related reviews\n",
      "388455 out of 500,000 related reviews\n",
      "373812 out of 500,000 related reviews\n",
      "414622 out of 500,000 related reviews\n",
      "414618 out of 500,000 related reviews\n",
      "433111 out of 500,000 related reviews\n",
      "107384 out of 500,000 related reviews\n"
     ]
    }
   ],
   "source": [
    "# There are multiple chunks to be read\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['review_id','useful','funny','cool'], axis=1)\n",
    "    # Renaming column name to avoid conflict with business overall star rating\n",
    "    chunk_review = chunk_review.rename(columns={'stars': 'review_stars'})\n",
    "    # Inner merge with edited business file so only reviews related to the business remain\n",
    "    chunk_merged = pd.merge(df_merged, chunk_review, on='business_id', how='inner')\n",
    "    # Show feedback on progress\n",
    "    print(f\"{chunk_merged.shape[0]} out of {size:,} related reviews\")\n",
    "    chunk_list.append(chunk_merged)\n",
    "# After trimming down the review file, concatenate all relevant data back to one dataframe\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6588460, 24)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load UserId Data (Massive dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 500000\n",
    "review = pd.read_json('yelp_academic_dataset_user.json', lines=True,\n",
    "                      dtype={'user_id':str,'name':str,\n",
    "                             'review_count':int},\n",
    "                      chunksize=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4073724 out of 500,000 related reviews\n",
      "1175245 out of 500,000 related reviews\n",
      "689031 out of 500,000 related reviews\n",
      "486388 out of 500,000 related reviews\n",
      "164072 out of 500,000 related reviews\n"
     ]
    }
   ],
   "source": [
    "# There are multiple chunks to be read\n",
    "chunk_list = []\n",
    "for chunk_review in review:\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['useful','elite','funny','cool','friends','yelping_since',\n",
    "                                      'fans','compliment_more','compliment_profile','compliment_hot',\n",
    "                                      'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "                                      'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "                                      'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "                                      'compliment_photos'], axis=1)\n",
    "    # Renaming column name to avoid conflict with business overall star rating\n",
    "    chunk_review = chunk_review.rename(columns={'name': 'username','review_count':'user_count'})\n",
    "    # Inner merge with edited business file so only reviews related to the business remain\n",
    "    chunk_merged = pd.merge(df, chunk_review, on='user_id', how='inner')\n",
    "    # Show feedback on progress\n",
    "    print(f\"{chunk_merged.shape[0]} out of {size:,} related reviews\")\n",
    "    chunk_list.append(chunk_merged)\n",
    "# After trimming down the review file, concatenate all relevant data back to one dataframe\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6588460, 27)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
       "       'latitude', 'longitude', 'stars', 'review_count', 'attributes',\n",
       "       'categories', 'hours', 'cat_kitchen', 'cat_type', 'state_name',\n",
       "       'latitude_state', 'longitude_state', 'PriceRange', 'AvgPrice',\n",
       "       'user_id', 'review_stars', 'text', 'date', 'username', 'user_count',\n",
       "       'average_stars'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data prep\n",
    "\n",
    "Select 2014-2021, so we have full years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004-10-14 02:57:52\n",
      "2021-01-28 15:23:52\n"
     ]
    }
   ],
   "source": [
    "print (df.date.min())\n",
    "print (df.date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = pd.Timestamp(2014,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the data\n",
    "df = df[(df['date']>= time_start)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014-01-01 00:01:51\n",
      "2021-01-28 15:23:52\n"
     ]
    }
   ],
   "source": [
    "print (df.date.min())\n",
    "print (df.date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5177322, 27)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites\\\\data'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites\\\\data'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataframe into equal big parts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = df[(df['date'].dt.year == 2014)]\n",
    "df_2015 = df[(df['date'].dt.year == 2015)]\n",
    "df_2016 = df[(df['date'].dt.year == 2016)]\n",
    "df_2017 = df[(df['date'].dt.year == 2017)]\n",
    "df_2018 = df[(df['date'].dt.year == 2018)]\n",
    "df_2019 = df[(df['date'].dt.year == 2019)]\n",
    "df_2020 = df[(df['date'].dt.year >= 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014.to_csv(\"yelp_reviews_RV_categories_2014.csv\", index=False)\n",
    "df_2015.to_csv(\"yelp_reviews_RV_categories_2015.csv\", index=False)\n",
    "df_2016.to_csv(\"yelp_reviews_RV_categories_2016.csv\", index=False)\n",
    "df_2017.to_csv(\"yelp_reviews_RV_categories_2017.csv\", index=False)\n",
    "df_2018.to_csv(\"yelp_reviews_RV_categories_2018.csv\", index=False)\n",
    "df_2019.to_csv(\"yelp_reviews_RV_categories_2019.csv\", index=False)\n",
    "df_2020.to_csv(\"yelp_reviews_RV_categories_2020.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
