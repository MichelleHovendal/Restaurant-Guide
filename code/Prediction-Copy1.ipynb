{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Miche\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = pd.read_csv('data/yelp_reviews_RV_categories_2014.csv')\n",
    "df_2015 = pd.read_csv('data/yelp_reviews_RV_categories_2015.csv')\n",
    "df_2016 = pd.read_csv('data/yelp_reviews_RV_categories_2016.csv')\n",
    "df_2017 = pd.read_csv('data/yelp_reviews_RV_categories_2017.csv')\n",
    "df_2018 = pd.read_csv('data/yelp_reviews_RV_categories_2018.csv')\n",
    "df_2019 = pd.read_csv('Restaurant-Guide/data/yelp_reviews_RV_categories_2019.csv')\n",
    "df_2020 = pd.read_csv('Restaurant-Guide/data/yelp_reviews_RV_categories_2020.csv')\n",
    "\n",
    "df = df_2014.append([df_2015, df_2016, df_2017, df_2018, df_2019, df_2020])\n",
    "# df = df.drop(['hours','address', 'username'],axis=1) #these coulmns are removed, since they are not used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1674264, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset='text')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.shape[0]\n",
    "MaxUsed = 100\n",
    "\n",
    "df = df.iloc[0:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of review\n",
    "df['reviewlength'] = df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of sentences\n",
    "df['sentences'] = df[\"text\"].str.count('\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#antal tal\n",
    "\n",
    "# procentdel af store bogstaver\n",
    "\n",
    "#procentdel af positive/negative meningsbærende ord --> Måske denne tages ud. Eller der laves liste med positive ord og en med negative\n",
    "\n",
    "\n",
    "#gennemsnit længde af review\n",
    "#one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "df['filtered_text'] = df.text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in df['filtered_text']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem the text\n",
    "porter_stemmer = PorterStemmer()\n",
    "df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['tokenized_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_text'] = df['stemmed_tokens'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of stars given\n",
    "min_review = min(df['review_stars'].value_counts())\n",
    "\n",
    "#dataframes for each star\n",
    "df1 = df.loc[(df['review_stars'] == 1)]\n",
    "df2 = df.loc[(df['review_stars'] == 2)]\n",
    "df3 = df.loc[(df['review_stars'] == 3)]\n",
    "df4 = df.loc[(df['review_stars'] == 4)]\n",
    "df5 = df.loc[(df['review_stars'] == 5)]\n",
    "\n",
    "#sample dataframe for each star\n",
    "df1 = df1.sample(n=min_review,axis='rows')\n",
    "df2 = df2.sample(n=min_review,axis='rows')\n",
    "df3 = df3.sample(n=min_review,axis='rows')\n",
    "df4 = df4.sample(n=min_review,axis='rows')\n",
    "df5 = df5.sample(n=min_review,axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced reviews\n",
    "df_word = pd.concat([df1, df2, df3, df4, df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = []\n",
    "WordCounter = Counter(\" \".join(df_word[\"stemmed_text\"]).split()).most_common(MaxUsed)\n",
    "for j in range(len(WordCounter)):\n",
    "    Word.insert(j,WordCounter[j][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words = np.zeros((N,MaxUsed))\n",
    "\n",
    "#one-hot encoding over the 100 most used words\n",
    "for index, row in df.iterrows():\n",
    "    for i in range(np.size(Words, axis=1)):\n",
    "        if Word[i] in df['stemmed_text'].iloc[index]:\n",
    "            Words[index,i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop attributes with text\n",
    "df = df.drop(['text','filtered_text','tokenized_text', 'stemmed_tokens',\n",
    "       'stemmed_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for encoding\n",
    "df_words = pd.DataFrame(Words)\n",
    "\n",
    "#Encoding column names are changed\n",
    "Names = list(range(0, MaxUsed))\n",
    "ColNames = [str(n) for n in Names]\n",
    "\n",
    "df_words.columns = ColNames\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The encoding is added to the dataset\n",
    "df = df.join(df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding for userID and businessID\n",
    "df['user_id'] = df['user_id'].astype('category')\n",
    "df['business_id'] = df['business_id'].astype('category')\n",
    "\n",
    "df['UserCat'] = df['user_id'].cat.codes\n",
    "df['BusinessCat'] = df['business_id'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode date\n",
    "df['date']= pd.to_datetime(df['date'])\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['minute'] = df['date'].dt.minute\n",
    "df['DayOfWeek'] = df['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df[['stars', 'review_count','review_stars','reviewlength', 'sentences', 'UserCat','BusinessCat', 'year', 'month', 'day', 'hour', 'minute', 'DayOfWeek', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.2),max_features=1)\n",
    "model.fit(df_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores']=model.decision_function(df_feature)\n",
    "df['anomaly']=model.predict(df_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['reviewlength', 'sentences', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', 'UserCat', 'BusinessCat', 'year', 'month', 'day', 'hour', 'minute', 'DayOfWeek'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['scores']>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Miche\\\\OneDrive - Danmarks Tekniske Universitet\\\\MMC\\\\2. Semester\\\\Social Data\\\\websites\\\\data'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = \"AnomalyDetection.csv\"\n",
    "df.to_csv(csv_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
